<html>
	<head>
	</head>
	<body>
		<p align="left">
		</p>
		<h5>Link to Webpage: <a href=https://cal-cs184-student.github.io/sp22-project-webpages-RupaDuggirala/proj3-1/index.html>https://cal-cs184-student.github.io/sp22-project-webpages-RupaDuggirala/proj3-1/index.html</a></h5>
		<br>
		<h1 align="left">Overview</h1>
		<p align="left">
			Through this project, we were able to implement a few key components of a physical renderer by building out the underlying path-tracing algorithm. In Parts I and II, we focused upon ray-scene intersection and a bounding volume hierarchy (or BVH) acceleration structure. We focused upon creating the most foundational components of the renderer in these parts, whether it be generating camera rays or pixel samples, implementing the ray-triangle intersection algorithm as per the Moller-Trumbore method, and creating and intersecting the BVH in order to enables efficiently traverse primitives in a scene after categorizing them based on location through an optimally chosen splitPoint. In Parts III, IV, and V, we developed various illumination algorithms to produce the realistic effect of physically based lighting and materials upon our images. We first worked on direct illumination, and created two versions of direct lighting function – one with uniform hemisphere sampling and the other with importance sampling of the lights. Then, we transitioned into indirect global illumination, and estimated the total radiance by recursively calculating its value with each bounce of a ray in the specified sample direction, for which Russian Roulette was utilized for unbiased termination. Finally, we added adaptive sampling to concentrate the samples in the more “difficult” components of the image and avoid using a high number of samples per pixel.
		</p>
		<p align="left">
			Overall, we have found that this project was invaluable in helping us understand the nuances behind path-tracing and what it takes to render a 3D image such that the overall illumination within it is reminiscent of reality. Many of the tasks in this project involved a careful analysis of the various components within an image (ie. triangles, pixel samples, light rays), so we learned how to carefully observe the changes in each one and rendered the images many times to validate our thought process and visualize the results of our implementations! This was collectively one of our biggest takeaways from the project – we now have a greater appreciation for what it takes to visually compose a realistic image because we were able to intuitively comprehend and implement the corresponding lighting and rendering techniques. 
		</p>
		<br>
		<h1 align="left">Part 1: Ray Generation and Scene Intersection</h1>
		<p align="left">
			The rendering pipeline that we built in this section of the project involved two key components: ray generation and the intersection of primitives. To generate camera rays in the generate_ray() function, we first converted the given x-y image coordinates into a 3D coordinate system by creating the vector (x, y, 1). To convert this vector into camera space and yield the correct coordinates, we found that a combined translation and scale transform was necessary. We first converted the hFov and vFov sensor parameters from degrees to radians, and calculated tan(0.5 * hFov) to be t_x and tan(0.5 * vFov) to be t_y – these were the quantities for the translation matrix. The initial 3D coordinates were then scaled by this translation matrix. To create the scale matrix, we calculated 2 * tan(0.5 * hFov) to be s_x and 2 * tan(0.5 * vFov) to be s_y. This matrix was multiplied by the output of the translation transform, and the resulting camera coordinates were the x-y coordinates of this resulting vector, as well as z = -1. We then generated the ray in the camera space, by creating a ray that was centered at (0, 0, 0) as its origin and with the camera coordinates as its direction. Finally, we transformed this ray to the world space by scaling its origin and direction with the c2w camera-to-world rotation matrix; origin was also scaled by pos, which is the camera position in the world space.
		</p>
		<p align="left">
			To address the intersection of primitives, we referenced the ray-triangle intersection algorithm – this will be explained in detail in the following paragraph. But at the core of this algorithm is the triangle, which is actually a primitive itself. As a result, after identifying that an intersection exists in the intersect() function, we manually set its parameters. One of these parameters is isect->primitive, which is set to “this” since the current triangle is the actual primitive that is being intersected. 
		</p>
		<p align="left">
			Overall, the triangle intersection algorithm that we implemented consists of two methods: has_intersection() and intersect(). In both functions, we applied the Moller-Trumbore ray-triangle intersection algorithm to parametrize the position of the intersection point and the distance from the ray origin to the triangle in terms of their Barycentric coordinates. Barycentric coordinates are utilized here as their positions are invariant even after the triangle and / or ray is moved or rotated, and since this is not the case for traditional Cartesian coordinates. Not only can this parameterization be thought of as a transformation that shifts the triangle from its original world space position to the origin, the intersection point is also effectively shifted from “x-y-z” space to “t-u-v” space (in which u,v are the Barycentric coordinates and t is the distance from the ray origin to the intersection point). Since the ray was effectively parametrized by t, a valid intersection between the ray and triangle is determined by the inequality t >= 0 and the condition that t lies within the range (min_t, max_t) of the input ray. Mathematically, this results in a system of linear equations, which can be solved by calculating the determinant as per Cramer’s Rule. In our implementation, the has_intersection() function utilizes this logic to test whether an intersection between the ray and triangle exists, while the intersect() function extends it a bit further to actually record the intersection point.  
		</p>
		<p align="left">
			The following screenshots of a few small .dae files show the outputs of our implementation, all with normal shading.
		</p>
		<img src="Task 1A.png">
		<img src="Task 1B.png">
		<img src="Task 1C.png">
		<img src="Task 1D.png">
		<br>
		<h1 align="left">Part 2: Bounding Volume Hierarchy</h1>
		<p align="left">
			We constructed a BVH (or bounding volume hierarchy) algorithm in this next section of the project, as it would serve the purpose of an acceleration structure that would allow us to partition objects during the process of ray-scene intersection much faster. To create the BVH, we first aimed to create an all-inclusive bounding box to “enclose” the list of primitives that was passed into the construct_bvh() function as input. This was done by adding each primitive’s bounding box to the all-inclusive one. We then extracted the centroid that was associated with each primitive. This was used to update the max and min values for each of the x, y, and z coordinates for each centroid (ie. minX, maxX, minY, maxY, minZ, and maxZ) if needed, and to add the current value of each coordinate to a running total in each direction (ie. totalX, totalY, totalZ). After this all-inclusive bounding box was created to “enclose” all of the primitives, we initialized a new node on the BVH binary tree. 
		</p>
		<p align="left">
			By this point, we were able to accumulate enough information from the provided inputs in order to analyze how to split the primitives into a “left” and “right” collection for faster traversal in the future. We first identified the longest axis (between the x-, y-, and z-axes) by finding the difference between the max and min values that were calculated earlier. We knew that the final splitIndex (or the point at which the original vector of primitives was split into two) had to be the midpoint of this longest axis, but in order for this index to be accurate, we had to incorporate a sorting algorithm. Based on which axis (x, y, or z) ended up being the longest one, we calculated the splitPoint as the total coordinate value (ie. totalX, totalY, or totalZ) in that direction divided by the total number of primitives. We chose this heuristic for the splitPoint for many reasons – foremost, by choosing the total coordinate values that correspond to the longest axis, we are able to guarantee that the corresponding split of the primitives into a “left” and “right” section confers the most benefit, or that it is the most balanced. This ensures that the subsequent nodes in the BVH tree are created such that they are efficient to traverse. Additionally, we divided this total value by the total number of primitives so that the splitPoint effectively represents a weighted average of the primitives along the chosen direction.
		</p>
		<p align="left">
			At this point, we found that it was necessary to sort the input list of primitives so that the splitIndex could be used accurately. We used a built-in partition method and split the list into two halves, conditioned on the fact that the splitIndex was the midpoint of the longest axis. To find this, we had to analyze the centroid values (either x, y, or z) of each primitive once again. The final component of the BVH construction algorithm was the base and recursive cases. We determined that if the total number of primitives was less than or equal to the input max_leaf_size, the current BVH node should be considered as a leaf node and the recursion can stop. If not, the recursion is two-fold, in that the left and right “branches” of the current BVH node are each set to a recursive call of construct_bvh() to iterate through one partition of the list of primitives as determined by splitIndex.
		</p>
		<p align="left">
			The following screenshots are of 4 images (max planck, lucy, dragon, bunny) of large .dae files with normal shading. These have a lot of triangles and therefore would not be feasible to render if not for BVH acceleration. 
		</p>
		<img src="Task 2A.png">
		<img src="Task 2B.png">
		<img src="Task 2C.png">
		<img src="Task 2D.png">
		<p align="left">
			We then ran a few experiments to compare rendering times on a few scenes that had moderately complex geometries. Foremost, we chose cow.dae. To render this scene without BVH acceleration, it took 49.3564 s. To render this scene with BVH acceleration, it only took 0.7565 s. Next, we chose teapot.dae. To render this scene without BVH acceleration, it took 19.8123 s. To render this scene with BVH acceleration, it only took 0.5276 s. Finally, we chose beetle.dae. To render this scene without BVH acceleration, it took 37.8114 s. To render this scene with BVH acceleration, it only took 0.5613 s. Overall, we found that BVH acceleration drastically reduced the amount of time that it took to render these scenes; the renders without this structure varied from anywhere between roughly 20 to 50 seconds, whereas the renders with it all occurred less than 1 second (or ¾ of a second, to be more precise). This massive difference in time complexity can be attributed to the fact that the BVH algorithm enables efficient traversal of primitives in a scene, by categorizing them based on location through an optimally chosen splitPoint.
		</p>
		<br>
	</body>
</html>
