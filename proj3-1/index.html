<!DOCTYPE html>
<!--
	Moon by GetTemplates.co
	URL: https://gettemplates.co
-->
<html lang="en">

<head>

	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Moon - Multipurpose Bootstrap 4 Template by GetTemplates.co</title>
	<meta name="description" content="Core HTML Project">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- External CSS -->
	<link rel="stylesheet" href="vendor/bootstrap/bootstrap.min.css">
	<link rel="stylesheet" href="vendor/select2/select2.min.css">
	<link rel="stylesheet" href="vendor/owlcarousel/owl.carousel.min.css">
	<link rel="stylesheet" href="vendor/lightcase/lightcase.css">

	<!-- Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400|Work+Sans:300,400,700" rel="stylesheet">

	<!-- CSS -->
	<link rel="stylesheet" href="css/style.min.css">
	<link rel="stylesheet" href="https://cdn.linearicons.com/free/1.0.0/icon-font.min.css">
	<link href="https://file.myfontastic.com/7vRKgqrN3iFEnLHuqYhYuL/icons.css" rel="stylesheet">

	<!-- Modernizr JS for IE8 support of HTML5 elements and media queries -->
	<script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.js"></script>

</head>
<body data-spy="scroll" data-target="#navbar-nav-header" class="single-layout">
	<div class="boxed-page">
		<nav id="gtco-header-navbar" class="navbar navbar-expand-lg py-4">
			<div class="container">
				<a class="navbar-brand d-flex align-items-center" href="../index.html">
					<span class="lnr lnr-moon"></span>
				</a>
				<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-nav-header" aria-controls="navbar-nav-header" aria-expanded="false" aria-label="Toggle navigation">
					<span class="lnr lnr-menu"></span>
				</button>
				<div class="collapse navbar-collapse" id="navbar-nav-header">
					<ul class="navbar-nav ml-auto">
						<li class="nav-item">
							<a class="nav-link" href="../index.html">Back to Home</a>
						</li>
					</ul>
				</div>
			</div>

		</nav>		<div class="jumbotron d-flex align-items-center" style="background-image: url(9_2.jpg)">
			<div class="container text-center">
				<h1 class="display-2 mb-4 single-blog-title">Project 3.1: Pathtracer</h1>
				<h3>Celeste Wu, Rupa Duggirala</h3>
			</div>
		</div>		<!-- Contact Form Section -->
		<section id="gtco-single-content" class="bg-white">
			<div class="container">
				<div class="section-content blog-content">
					<div class="row">
						<!-- Single Content Holder -->
						<div class="col-md-8 offset-md-2 mt-4">
							<h5>Link to Webpage: <a href=https://cal-cs184-student.github.io/sp22-project-webpages-RupaDuggirala/proj3-1/index.html>https://cal-cs184-student.github.io/sp22-project-webpages-RupaDuggirala/proj3-1/index.html</a></h5>
							<!-- <img class="float-left" width="320px" src="img/blog-4.jpg" alt=""> -->
							<br>
							<h1 align="left">Overview</h1>
							<p align="left">
								Through this project, we were able to implement a few key components of a physical renderer by building out the underlying path-tracing algorithm. In Parts I and II, we focused upon ray-scene intersection and a bounding volume hierarchy (or BVH) acceleration structure. We focused upon creating the most foundational components of the renderer in these parts, whether it be generating camera rays or pixel samples, implementing the ray-triangle intersection algorithm as per the Moller-Trumbore method, and creating and intersecting the BVH in order to enables efficiently traverse primitives in a scene after categorizing them based on location through an optimally chosen splitPoint. In Parts III, IV, and V, we developed various illumination algorithms to produce the realistic effect of physically based lighting and materials upon our images. We first worked on direct illumination, and created two versions of direct lighting function – one with uniform hemisphere sampling and the other with importance sampling of the lights. Then, we transitioned into indirect global illumination, and estimated the total radiance by recursively calculating its value with each bounce of a ray in the specified sample direction, for which Russian Roulette was utilized for unbiased termination. Finally, we added adaptive sampling to concentrate the samples in the more “difficult” components of the image and avoid using a high number of samples per pixel.
							</p>
							<p align="left">
								Overall, we have found that this project was invaluable in helping us understand the nuances behind path-tracing and what it takes to render a 3D image such that the overall illumination within it is reminiscent of reality. Many of the tasks in this project involved a careful analysis of the various components within an image (ie. triangles, pixel samples, light rays), so we learned how to carefully observe the changes in each one and rendered the images many times to validate our thought process and visualize the results of our implementations! This was collectively one of our biggest takeaways from the project – we now have a greater appreciation for what it takes to visually compose a realistic image because we were able to intuitively comprehend and implement the corresponding lighting and rendering techniques. 
							</p>
							<br>
							<h1 align="left">Part 1: Ray Generation and Scene Intersection</h1>
							<p align="left">
								The rendering pipeline that we built in this section of the project involved two key components: ray generation and the intersection of primitives. To generate camera rays in the generate_ray() function, we first converted the given x-y image coordinates into a 3D coordinate system by creating the vector (x, y, 1). To convert this vector into camera space and yield the correct coordinates, we found that a combined translation and scale transform was necessary. We first converted the hFov and vFov sensor parameters from degrees to radians, and calculated tan(0.5 * hFov) to be t_x and tan(0.5 * vFov) to be t_y – these were the quantities for the translation matrix. The initial 3D coordinates were then scaled by this translation matrix. To create the scale matrix, we calculated 2 * tan(0.5 * hFov) to be s_x and 2 * tan(0.5 * vFov) to be s_y. This matrix was multiplied by the output of the translation transform, and the resulting camera coordinates were the x-y coordinates of this resulting vector, as well as z = -1. We then generated the ray in the camera space, by creating a ray that was centered at (0, 0, 0) as its origin and with the camera coordinates as its direction. Finally, we transformed this ray to the world space by scaling its origin and direction with the c2w camera-to-world rotation matrix; origin was also scaled by pos, which is the camera position in the world space.
							</p>
							<p align="left">
								To address the intersection of primitives, we referenced the ray-triangle intersection algorithm – this will be explained in detail in the following paragraph. But at the core of this algorithm is the triangle, which is actually a primitive itself. As a result, after identifying that an intersection exists in the intersect() function, we manually set its parameters. One of these parameters is isect->primitive, which is set to “this” since the current triangle is the actual primitive that is being intersected. 
							</p>
							<p align="left">
								Overall, the triangle intersection algorithm that we implemented consists of two methods: has_intersection() and intersect(). In both functions, we applied the Moller-Trumbore ray-triangle intersection algorithm to parametrize the position of the intersection point and the distance from the ray origin to the triangle in terms of their Barycentric coordinates. Barycentric coordinates are utilized here as their positions are invariant even after the triangle and / or ray is moved or rotated, and since this is not the case for traditional Cartesian coordinates. Not only can this parameterization be thought of as a transformation that shifts the triangle from its original world space position to the origin, the intersection point is also effectively shifted from “x-y-z” space to “t-u-v” space (in which u,v are the Barycentric coordinates and t is the distance from the ray origin to the intersection point). Since the ray was effectively parametrized by t, a valid intersection between the ray and triangle is determined by the inequality t >= 0 and the condition that t lies within the range (min_t, max_t) of the input ray. Mathematically, this results in a system of linear equations, which can be solved by calculating the determinant as per Cramer’s Rule. In our implementation, the has_intersection() function utilizes this logic to test whether an intersection between the ray and triangle exists, while the intersect() function extends it a bit further to actually record the intersection point.  
							</p>
							<p align="left">
								The following screenshots of a few small .dae files show the outputs of our implementation, all with normal shading.
							</p>
							<img src="Task 1A.png">
							<p align="left">
								The following screenshots show each step, from the original 6 control points to the final evaluated point.
							</p>
							<img src="Task 1A.png">
							<img src="Task 1B.png">
							<img src="Task 1C.png">
							<img src="Task 1D.png">
							<br>
							<h1 align="left">Part 2: Bounding Volume Hierarchy</h1>
							<p align="left">
								We constructed a BVH (or bounding volume hierarchy) algorithm in this next section of the project, as it would serve the purpose of an acceleration structure that would allow us to partition objects during the process of ray-scene intersection much faster. To create the BVH, we first aimed to create an all-inclusive bounding box to “enclose” the list of primitives that was passed into the construct_bvh() function as input. This was done by adding each primitive’s bounding box to the all-inclusive one. We then extracted the centroid that was associated with each primitive. This was used to update the max and min values for each of the x, y, and z coordinates for each centroid (ie. minX, maxX, minY, maxY, minZ, and maxZ) if needed, and to add the current value of each coordinate to a running total in each direction (ie. totalX, totalY, totalZ). After this all-inclusive bounding box was created to “enclose” all of the primitives, we initialized a new node on the BVH binary tree. 
							</p>
							<p align="left">
								By this point, we were able to accumulate enough information from the provided inputs in order to analyze how to split the primitives into a “left” and “right” collection for faster traversal in the future. We first identified the longest axis (between the x-, y-, and z-axes) by finding the difference between the max and min values that were calculated earlier. We knew that the final splitIndex (or the point at which the original vector of primitives was split into two) had to be the midpoint of this longest axis, but in order for this index to be accurate, we had to incorporate a sorting algorithm. Based on which axis (x, y, or z) ended up being the longest one, we calculated the splitPoint as the total coordinate value (ie. totalX, totalY, or totalZ) in that direction divided by the total number of primitives. We chose this heuristic for the splitPoint for many reasons – foremost, by choosing the total coordinate values that correspond to the longest axis, we are able to guarantee that the corresponding split of the primitives into a “left” and “right” section confers the most benefit, or that it is the most balanced. This ensures that the subsequent nodes in the BVH tree are created such that they are efficient to traverse. Additionally, we divided this total value by the total number of primitives so that the splitPoint effectively represents a weighted average of the primitives along the chosen direction.
							</p>
							<p align="left">
								At this point, we found that it was necessary to sort the input list of primitives so that the splitIndex could be used accurately. We used a built-in partition method and split the list into two halves, conditioned on the fact that the splitIndex was the midpoint of the longest axis. To find this, we had to analyze the centroid values (either x, y, or z) of each primitive once again. The final component of the BVH construction algorithm was the base and recursive cases. We determined that if the total number of primitives was less than or equal to the input max_leaf_size, the current BVH node should be considered as a leaf node and the recursion can stop. If not, the recursion is two-fold, in that the left and right “branches” of the current BVH node are each set to a recursive call of construct_bvh() to iterate through one partition of the list of primitives as determined by splitIndex.
							</p>
							<p align="left">
								The following screenshots are of 4 images (max planck, lucy, dragon, bunny) of large .dae files with normal shading. These have a lot of triangles and therefore would not be feasible to render if not for BVH acceleration. 
							</p>
							<img src="Task 2A.png">
							<img src="Task 2B.png">
							<img src="Task 2C.png">
							<img src="Task 2D.png">
							<p align="left">
								We then ran a few experiments to compare rendering times on a few scenes that had moderately complex geometries. Foremost, we chose cow.dae. To render this scene without BVH acceleration, it took 49.3564 s. To render this scene with BVH acceleration, it only took 0.7565 s. Next, we chose teapot.dae. To render this scene without BVH acceleration, it took 19.8123 s. To render this scene with BVH acceleration, it only took 0.5276 s. Finally, we chose beetle.dae. To render this scene without BVH acceleration, it took 37.8114 s. To render this scene with BVH acceleration, it only took 0.5613 s. Overall, we found that BVH acceleration drastically reduced the amount of time that it took to render these scenes; the renders without this structure varied from anywhere between roughly 20 to 50 seconds, whereas the renders with it all occurred less than 1 second (or ¾ of a second, to be more precise). This massive difference in time complexity can be attributed to the fact that the BVH algorithm enables efficient traversal of primitives in a scene, by categorizing them based on location through an optimally chosen splitPoint.
							</p>
							<br>
							<h1 align="left">Part 3: Direct Illumination</h1>
							<p align="left">
								In this section, we implemented two different types of the direct lighting function: one with uniform hemisphere sampling, and the other with importance sampling. For the estimate_direct_lighting_hemisphere() function, we first sought to estimate how much light arrived at the intersection point utilizing the Monte Carlo estimation method. This involved creating a for loop that would iterate num_samples times, as this was the total number of directions in the hemisphere that we sought to sample. In each iteration of the loop, we sampled w_j, a new sample from the hemisphereSampler. This was converted into world space using the provided object-to-world conversion matrix, and a corresponding worldRay_w_j was created to represent a new incoming ray direction in the hemisphere. We then utilized the bvh->intersect() method that we defined earlier to determine if this worldRay_w_j was created such that it could have a valid intersection; if so, we checked if the emission value of the surface material was zero, as this represented a light source. We only sought to add to the estimator sum if a new ray with its origin at hit_p pointing in the sampled direction intersects with the light source (if it exists at all) – if so, this quantity composed a numerator for the reflection equation. This numerator was divided by the denominator 1 / 2 * PI, and this value was added to the collective estimator sum. After all directions in the hemisphere were sampled (or the for loop iterated num_samples times), we scaled the estimator by 1 / num_samples, as that result represented a weighted average of all of the samples that is reminiscent of the total outgoing light, which is the final output of this equation. 
							</p>
							<p align="left">
								For the estimate_direct_lighting_importance() function, we once again sought to estimate how much light arrived at the intersection point utilizing the Monte Carlo estimation method. In this implementation, we iterated through all of the lights in the scene using the scene->lights vector. In each iteration of the loop, we checked to see if the light was a delta light (or if it was a point light source). If so, we knew that we only needed to sample it once and directly utilized the provided sample_L() function. This returned w_i, or the sampled direction between hit_p and the light source, which we converted into object space using the provided world-to-object conversion matrix. A corresponding shadowRay was created to represent a new incoming ray direction between the hit point and the light, as this would inevitably create a shadow. Once again, we utilized the bvh->intersect() method that we defined earlier to determine if this shadowRay was created such that it could have a valid intersection, and calculated the cosine as a dot product of the angle between w_i and the Vector3D(0, 0, 1). We found that the estimator sum should only be updated if there was no intersection and if the cosine result was positive – if so, the numerator of the quantity was found as per the reflection equation, or a product of the cosine, emitted radiance from sample_L(), and the reflectance between w_out and w_i; while the denominator was the pdf as calculated by sample_L(). This quantity was calculated and added to the estimator, and subsequently to the final output L_out. On the other hand, if the light was not a delta light (or not a point light source), we had to create a for loop that would iterate ns_area_light times, as this was the total number of samples that should be taken for such a light. The process of utilizing the sample_L() function to find the emitted radiance, calculating the corresponding shadowRay, and only adding to the estimator sum if there was no intersection and if the cosine result was positive were all the same from before. The only difference in this branch of the function was that the estimator sum was scaled by 1 / ns_area_light before being added to the final output L_out, as this result represented a weighted average of all of the samples (just like with hemisphere sampling). 
							</p>
							<p align="left">
								The following screenshots are rendered with these two implementations of the direct lighting function. The top one (for each file) is rendered with uniform hemisphere sampling, while the bottom one is rendered with importance sampling.
							</p>
							<img src="Task 3A.png">
							<img src="Task 3B.png">
							<img src="Task 3C.png">
							<img src="Task 3D.png">
							<p align="left">
								In an experiment, we focused upon one particular scene, and ensured that it contained at least one area light. The following 4 screenshots demonstrate how we compared the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling. 
							</p>
							<img src="Task 3E.png">
							<img src="Task 3F.png">
							<img src="Task 3G.png">
							<img src="Task 3H.png">
							<p align="left">
								Overall, we found that the images that were rendered with uniform hemisphere sampling are a lot more coarse in texture – for both bunny.dae and CBspheres_lambertian.dae, we can visually see that the background is slightly grainy, which is created by a mix of black and either gray, red, or blue pixels (depending on the specific component). As a result, the shadows and other different types of shading that are present in these images do not appear to be as sharp. Additionally, the top light in the Cornell box appears to be slightly blurred, which means the light effectively “bleeds” into the surrounding black wall. On the other hand, we found that the images that were rendered with importance lighting sampling do not look as coarse from a texture standpoint – for both bunny.dae and CBspheres_lambertian.dae, we can visually see that the background is not grainy, and that it appears to be created entirely out of gray pixels. As a result, the shadows and other different types of shading that are present in these images appear to be sharp and well-defined. Additionally, the corners of the top light in the Cornell box here appear to be sharp, which means that the light and the black wall look more like discrete components.
							</p>
							<br>
							<h1 align="left">Part 4: Global Illumination</h1>
							<p align="left">
								We followed the suggestions and drew out two same triangles which were going to be the target of edge flipping, listing all the components: edges, halfedges, faces and vertices. We first worked on changing the root vertex of the two halfedges on that edge, then the next() assignments, which we used extensively in the parts for assigning faces to halfedges. Finally, we changed the halfedge assignments in the vertices and faces to match that of the halfedge which we accessed them with. That way, we can ensure a sort of reciprocity (ie. if we accessed a vertex/face using a halfedge, then that halfedge can be assigned to that vertex/face since all halfedge information is accurate at this point). Picking the order of doing things turned out to be rather important for staying sane, and it took a few tries to optimize. It was also helpful to realize that edge-halfedge and twin() relationships don’t change, which matched the unchanged results we saw when we commented out those assignments.
							</p>
							<p align="left">
								Implementation tricks: For the next() reassignments, laying out all the edges in advance and naming them after the diagram we were referring to definitely helped. What also helped was using the “reciprocity” trick above for assigning halfedges to vertices/faces, after recognizing that once we have an accurate way to traverse the halfedges, we are good to rely on the halfedge information.
							</p>
							<p align="left">
								Debugging tricks: isolating reassignments by reordering them to see whether our bugs were dependent on that, which would indicate some reliance on data that is being changed.
							</p>
							<p align="left">
								Debugging journey: First we had a bug where half of the face would be gone and we could see through to the otherside. We quickly realized that we forgot to re-assign the halfedge’s faces and the face’s halfedges. Then, we were trying to avoid assignment / pointer issues by defining new Halfedge variables to be placeholders, but it turns out that mucked everything up. Two of the edges that are not being flipped decided to overlap with the other two edges opposite of them. When we switched back to using the iterator to perform the assignments, everything actually went okay and it fixed the silly aforementioned bug.
							</p>
							<p align="left">
								Here are some edge flips!
							</p>
							<img src="Task 4A.png">
							<img src="Task 4B.png">
							<img src="Task 4C.png">
							<img src="Task 4D.png">
							<img src="Task 4E.png">
							<img src="Task 4F.png">
							<br>
							<h1 align="left">Part 5: Adaptive Sampling</h1>
							<p align="left">
								To implement this task, we drew out two adjacent triangles once again – this time, with the edges, halfedges, faces and vertices that would be present after the shared edge between the two triangles was split. After doing this, we first calculated how to add in the new vertex at the midpoint of the shared edge. We then were able to form halfedges (and assign edges and vertices to each one) to connect this new vertex to all four of the original vertices. At this point, we focused upon the fact that the edge split operation transformed two adjacent triangle faces into four – we analyzed each triangle individually, ensuring that the next and twin pointers for the edges and the faces for each halfedge were precisely modified to reflect this change. In accordance with the reciprocity approach from task 4, we had to ensure that the relationship between a vertex or face and their corresponding halfedge was assigned from both perspectives; this was the final task at hand in order to update all of the new components and ensure that the edge split was done successfully.
							</p>
							<p align="left">
								What helped the most when we were implementing or debugging this task was to draw out the result of the edge split operation on the original two triangles, and to observe (and subsequently implement) all of the edges, halfedges, faces and vertices that would be present in the final state. Initially, we found that it was difficult to keep track of all of these new components, but we chose to split up each task into segments (ie. changing the next pointers for each edge or changing the faces for each halfedge) that were repeated for each of the four resulting triangle faces. Being able to “view” the task through this lens and reference the reciprocity approach from task 4 allowed us to ensure that all of the new and old attributes were being assigned correctly.
							</p>
							<p align="left">
								Here are two screenshots of a mesh: the first depicts the original state before an edge split, and the second is after the edge split.
							</p>
							<img src="Task 5A.png">
							<img src="Task 5B.png">
							<p align="left">
								The next two screenshots display the mesh before and after multiple edge splits along two columns at its center.
							</p>
							<img src="Task 5C.png">
							<img src="Task 5D.png">
							<p align="left">
								The next sequence of screenshots displays the mesh before and after a combination of multiple edge splits and edge flips.
							</p>
							<img src="Task 5C.png">
							<img src="Task 5E.png">
							<p align="left">
								Our debugging journey was quite eventful – initially, we attempted to populate the new and existing edges simultaneously and then modify the faces that correspond to each halfedge. Yet through this approach, we found that we were missing many pointer assignments and did not accurately create all of the new components or update the old ones; therefore, the split operation did not work for us at all. After shifting our focus to the final four adjacent triangles and segmenting each sub-task accordingly, we found that it was easier to keep track of and implement the changes to all of the edges, halfedges, faces and vertices such that they would be in the ideal final state.
							</p>
							<br>
							<h1 align="left">Conclusion</h1>
							<p align="left">
								Through this project, we were able to implement a few key components of a physical renderer by building out the underlying path-tracing algorithm. In Parts I and II, we focused upon ray-scene intersection and a bounding volume hierarchy (or BVH) acceleration structure. We focused upon creating the most foundational components of the renderer in these parts, whether it be generating camera rays or pixel samples, implementing the ray-triangle intersection algorithm as per the Moller-Trumbore method, and creating and intersecting the BVH in order to enables efficiently traverse primitives in a scene after categorizing them based on location through an optimally chosen splitPoint. In Parts III, IV, and V, we developed various illumination algorithms to produce the realistic effect of physically based lighting and materials upon our images. We first worked on direct illumination, and created two versions of direct lighting function – one with uniform hemisphere sampling and the other with importance sampling of the lights. Then, we transitioned into indirect global illumination, and estimated the total radiance by recursively calculating its value with each bounce of a ray in the specified sample direction, for which Russian Roulette was utilized for unbiased termination. Finally, we added adaptive sampling to concentrate the samples in the more “difficult” components of the image and avoid using a high number of samples per pixel.
							</p>

							<hr>
							<h5>
								Moon by GetTemplates.co <br>
								URL: https://gettemplates.co
							</h5>
						</div>
						<!-- End of Contact Form Holder -->
					</div>
				</div>
			</div>
		</section>
		<!-- End of Contact Form Section -->		<footer class="mastfoot mb-3 bg-white py-4 border-top">
			<div class="inner container">
				<div class="row">
					<div class="col-md-6 d-flex align-items-center justify-content-md-start justify-content-center">
						<p class="mb-0">&copy; 2019 Moon. All Right Reserved. Design by <a href="https://gettemplates.co" target="_blank">GetTemplates.co</a>.</p>
					</div>

				</div>
			</div>
		</footer>
	</div>

	</div>
	<!-- External JS -->
	<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1/jquery.js"></script>
	<script src="vendor/bootstrap/popper.min.js"></script>
	<script src="vendor/bootstrap/bootstrap.min.js"></script>
	<script src="vendor/select2/select2.min.js "></script>
	<script src="vendor/owlcarousel/owl.carousel.min.js"></script>
	<script src="vendor/isotope/isotope.min.js"></script>
	<script src="vendor/lightcase/lightcase.js"></script>
	<script src="vendor/waypoints/waypoint.min.js"></script>
	<script src="vendor/countTo/jquery.countTo.js"></script>

	<!-- Main JS -->
	<script src="js/app.min.js "></script>
	<script src="//localhost:35729/livereload.js"></script>
</body>
